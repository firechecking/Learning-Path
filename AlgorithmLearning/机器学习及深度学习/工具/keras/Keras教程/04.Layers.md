# Layers
## Layer简介
所有的Keras Layer都具有如下方法：

1. layer.get_weights()：获取层的权重
1. layer.set_weights(weigths)：使用numpy array设置权重（weights必须和get_weights()具有相同的shape）
1. layer.get_config()：返回layer的设置，并可以通过设置重新实例化layer

```
layer = Dense(32)
config = layer.get_config()
reconstructed_layer = Dense.from_config(config)
```
或：

```
from keras import layers

config = layer.get_config()
layer = layers.deserialize({'class_name': layer.__class__.__name__,
                            'config': config})
```
如果layer只有1个输入node，可以通过一下属性获取输入tensor、输出tensor、输入输出shape：
1. layer.input
1. layer.output
1. layer.input_shape
1. layer.output_shape

如果具有多个输入node，需要使用以下方法：
1. layer.get_input_at(node_index)
1. layer.get_output_at(node_index)
1. layer.get_input_shape_at(node_index)
1. layer.get_output_shape_at(node_index)

## 核心Layer
### Dense
```
keras.layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)
```
Dense为全连接层，其执行的操作如下：`output = activation(dot(input,kernel)+bias)`，上式中，activation是参数传递的激活函数，kernel是权重矩阵，bias是偏置向量（参数use_bias=True时bias才有效）
* **Note**: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with `kernel`.（如果输入维数大于2，在进行全连接点积计算前，会首先对input进行展开）

**Example**

```
# 惯序模型
model = Sequential()
model.add(Dense(32, input_shape=(16,)))
# 模型接收（*，16）作为输入，并产生（*，32）的输出。其中*表示数据量

# 除第一层外，后续层不需要制定输入shape
model.add(Dense(32))
```
**参数**
* units：正整数，表示输出数据的维数
* activation：激活函数（[activations](https://keras.io/activations/))），如果不指定，默认不使用任何激活函数
* use_bias：布尔值，设定是否使用偏置向量
* kernel_initializer：权重矩阵的初始化器（[initializer](https://keras.io/initializers/)）
* bias_initializer:  偏置向量的初始化器([initializers](https://keras.io/initializers/))
* kernel_regularizer：权重矩阵的正则化函数([regularizer](https://keras.io/regularizers/))
* bias_regularizer：偏置向量的正则化函数([regularizer](https://keras.io/regularizers/))
* activity_regularizer：激活函数输出的正则化函数([regularizer](https://keras.io/regularizers/))
* kernel_constraint：权重矩阵的约束函数([constraints](https://keras.io/constraints/))
* bias_constraint：偏置向量的约束函数([constraints](https://keras.io/constraints/))

**input shape**
输入tensor包括输入的batch_size，然后是输入。对于n维tensor，shape是(batch_size, ..., input_dim)，对于最常用的2D tensor，输入shape是(batch_size, input_dim)

**output shape**
n维tensor的shape是(batch_size, ..., units)，对于shape是(batch_size, input_dim)的2D输入，输出的shape是(batch_size, units)。

举例：
* input_shape=(?,16)，output_shape=(?,units)
* input_shape=(?,32,16)，output_shape=(?,32,units)

### Activation
```
keras.layers.Activation(activation)
```
Activation表示添加一个激活函数到模型中

**参数**
1. activation：激活函数名称

**input shape**
当Activation为第一层时，需要传递一个整数的tuple类型给input_shape参数来指定输入数据维数（input_shape不包括batch_size）

**output shape**
和input shape的维数相同

### Dropout
```
keras.layers.Dropout(rate, noise_shape=None, seed=None)
```
dropout层通过一定概率（rate）来设置输入节点为0，从而避免过拟合（dropout层只在训练阶段生效）

**参数**
1. rate：0-1之间的实数，设置输入节点的丢弃概率
1. noise_shape（`还不太理解！`）：1D integer tensor representing the shape of the binary dropout mask that will be multiplied with the input. For instance, if your inputs have shape`(batch_size, timesteps, features)` and you want the dropout mask to be the same for all timesteps, you can use `noise_shape=(batch_size, 1, features)`
1. seed：整数表示的随机种子

**参考**
*   [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)

### Flatten
```
keras.layers.Flatten()
```
Flatten层对输入数据进行展开，但不会影响batch size

**Example**

```
model = Sequential()
model.add(Conv2D(64, 3, 3,
                 border_mode='same',
                 input_shape=(3, 32, 32)))
# now: model.output_shape == (None, 64, 32, 32)

model.add(Flatten())
# now: model.output_shape == (None, 65536)
```
### Input
```
keras.engine.topology.Input()
```
Input用于实例化一个Keras Tensor，Keras tensor的类型取决于选用的后端（Theano、TensorFlow、CNTK），有了inputs tensor和outputs tensor，就可以创建keras model。

比如知道a、b、c这3个tensor，那么可以用如下方式创建模型
```
model = Model(input=[a, b], output=c)
```
**属性**
1. _keras_shape：tensor的shape
1. _keras_history：最后一层网络，可以通过最后一层网络递归的访问到整个模型的层

**参数**
1. shape：输入数据的shape（不包括batch size），如shape=(32,)表示输入数据是32维向量
1. batch_shape：输入数据shape（包括batch size），如batch_shape=(10,32)表示输入数据是batch为10的32维向量，batch_shape=(None,32)表示输入数据是任意batch的32维向量
1. name：表示层代号的字符串，在同一个模型中，name必须是唯一的。当不指定name参数时，会自动生成一个name
1. dtype：以字符串表示的数据类型（float32，float64，int32等）
1. sparse：布尔值，表示创建的tensor是否是稀疏的
1. tensor：是否使用一个现存的tensor来设置Input层的值，如果设置tensor参数，就不会创建占位的tensor


**返回值**
tensor

### Reshape
```
keras.layers.Reshape(target_shape)
```
将输入转换成指定的shape并输出

**参数**
1. target_shape：整数型tuple制定输出的shape，不包括batch size
1. input_shape：如果是模型的第一层，需要制定input_shape（不包括batch size）

**output shape**
(batch_size,)+target_shape

**Example**
```python
model = Sequential()
model.add(Reshape((3, 4), input_shape=(12,)))
# now: model.output_shape == (None, 3, 4)
# note: `None`指batch size

# as intermediate layer in a Sequential model
model.add(Reshape((6, 2)))
# now: model.output_shape == (None, 6, 2)

# 可以用-1指定自动计算的维度值
model.add(Reshape((-1, 2, 2)))
# now: model.output_shape == (None, 3, 2, 2)
```
### Permute
```
keras.layers.Permute(dims)
```
Permute层根据给定的模式来重排列输入数据的维度

**Example**
```
model = Sequential()
model.add(Permute((2, 1), input_shape=(10, 64)))
# Permute(2,1)将输入的维度1，2互换
# now: model.output_shape == (None, 64, 10)
# note: `None` is the batch dimension
```
**参数**
1. dims：整数tuple，表示重排列方式，不包括batch size，因此从1开始
1. input_shape：当在模型第一层时，需要制定input_shape（不包括batch size）

**output shape**
输出的shape和输入的shape一样，但是按照dims对维数进行了重排列

### RepeatVector
```
keras.layers.RepeatVector(n)
```
将输入重复n次

**Example**
```
model = Sequential()
model.add(Dense(32, input_dim=32))
# now: model.output_shape == (None, 32)
# note: `None` is the batch dimension

model.add(RepeatVector(3))
# now: model.output_shape == (None, 3, 32)
```
**参数**
1. n：整数，表示重复次数

**input shape**
(batch_size,features)
RepeatVector的input shape只支持一维
**output shape**
(batch_size,n,features)

### Lambda
```
keras.layers.Lambda(function, output_shape=None, mask=None, arguments=None)
```
将python表达式包装成Layer

**Example**
```
# 添加一个y = x^2的层
model.add(Lambda(lambda x: x ** 2))
```
```python
# add a layer that returns the concatenation
# of the positive part of the input and
# the opposite of the negative part

def antirectifier(x):
    x -= K.mean(x, axis=1, keepdims=True)
    x = K.l2_normalize(x, axis=1)
    pos = K.relu(x)
    neg = K.relu(-x)
    return K.concatenate([pos, neg], axis=1)

def antirectifier_output_shape(input_shape):
    shape = list(input_shape)
    assert len(shape) == 2  # only valid for 2D tensors
    shape[-1] *= 2
    return tuple(shape)

model.add(Lambda(antirectifier,
                 output_shape=antirectifier_output_shape))
```
**参数**
1. function：进行计算的函数，函数以input tensor为第一个参数
1. output_shape：只有在后端使用Theano时有效，表示function的输出shape，如果是tuple，则output_shape=(input_shape[0],)+output_shape，如果是函数，则output_shape=f(input_shape)
1. arguments：字典类型，表示需要传递给function的参数列表

**input shape**
当为第一层时，可以通过arguments传递input_shape给模型

**output shape**
如果是Theano，则由output_shape参数决定；如果是TensorFlow，则自动计算

### ActivityRegularization
```
keras.layers.ActivityRegularization(l1=0.0, l2=0.0)
```
对损失函数进行正则化

**参数**
1. l1：L1正则化参数
1. l2：L2正则化参数

**input shape**
使用input_shape制定第一层的输入shape

**output shape**
和input shape相同

### Masking
```
keras.layers.Masking(mask_value=0.0)
```
对input tensor的每一帧（tensor的维度1为帧，维度0为batch），如果input tensor的某一帧所有值都等于mask_value，那么这一帧在后续的layer中就会被masked（跳过），如果Masking没有后续的layer，则会报错

**Example**
如果输入tensor为x，x的shape为(samples, timesteps, features)，x需要作为一个LSTM网络的输入，但是相对timestep=3和5的值进行mask，那么需要如下处理
1. 设置x[:,3,:] = 0, x[:,5,:] = 0;
1. 在LSTM之前插入一个Masking层，mask_value=0:
```
model = Sequential()
model.add(Masking(mask_value=0., input_shape=(timesteps, features)))
model.add(LSTM(32))
```
## 卷积层
### Conv1D
```
keras.layers.Conv1D(filters, kernel_size, strides=1, padding='valid', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)
```
1维卷积（如：时域卷积）
Conv1D创建1个1维的卷积核，如果use_bias=True，在outputs中会加上偏置，如果activation不为None，在outputs上还会进行激活函数运算。
当作为模型第一层时，还需要提供input_shape参数（tuple类型的整数或None），如（10，128）表示10个128维向量序列，（None，128）表示可变长度的128位向量。

**参数**
1. filters：整数，卷积核个数
1. kernel_size：整数或只有1个整数元素的tuple/list，表示1位卷积核的窗口长度
1. strides：整数或只有1个整数元素的tuple/list，表示卷积窗口滚动步长。注意：strides！=1与dilation_rate!=1有冲突
1. padding："valid"或"causal"或"same"，表示卷积核在input的边界处理方式。
	1. "valid"：不扩展input
	1. "same"：扩展input边界，使output和input具有相同的长度
	1. "causal"：`暂时不清楚！`
1. dilation_rate：整数或只有1个整数元素的tuple/list，dilated convolution中的膨胀比例（`暂时不懂！`），注意：strides！=1与dilation_rate!=1有冲突
1. activation：激活函数，不指定则为None
1. use_bias：布尔型，表示是否使用偏置向量
1. kernel_initializer：权重矩阵的初始化器
1. bias_initializer：偏置向量的初始化器
1. kernel_regularizer：应用到权重矩阵的正则化方程
1. bias_regularizer：应用到偏置向量的正则化方程
1. activity_regularizer：应用到输出的正则化函数
1. kernel_constraint：应用到权重矩阵的约束函数
1. bias_constraint：应用到偏执向量的约束函数

**input shape**
一个三维的tensor：（batch_size, steps, input_dim）

**output shape**
一个三维的tensor：（batch_size, new_steps, filters），steps的值根据padding和strides值的不同，会有变化

### Conv2D
```
keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)
```
二维卷积层，如图像卷积

当为模型第一层时，需要提供input_shape，如input_shape=(128, 128, 3)表示一个128x128像素的RGB图像（data_format = "channels_last"）

**参数**
（除以下特殊说明外，参考Conv1D）
1.  data_format：“channels_last”或“channels_first”，data_format的默认值在"~/.keras/keras.json"中，如果没有修改过，则默认为"channels_last"

**input shape**
channels_first: （samples, channels, rows, cols）
channels_last: （samples, rows, cols, channels）

**output shape**
channels_first: （samples, filters, new_rows, new_cols）
channels_last: （samples, new_rows, new_cols, filters）

### SeparableConv2D
```
keras.layers.SeparableConv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, depth_multiplier=1, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, pointwise_regularizer=None, bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None)
```
深度方向可分离的2维卷积

可分离卷积首先按深度方向进行卷积（对每个输入通道分别卷积），然后逐点进行卷积，将上一步的卷积结果混合到输出通道中。参数`depth_multiplier`控制了在depthwise卷积（第一步）的过程中，每个输入通道信号产生多少个输出通道。

直观来说，可分离卷积可以看做讲一个卷积核分解为两个小的卷积核，或看作Inception模块的一种极端情况。

当作为模型的第一层是，需要提供input_shape参数

**input shape**
data_format="channels_first"：（batch, channels, rows, cols）
data_format="channels_last"：（batch, rows, cols, channels）

**output shape**
data_format="channels_first"：（batch, filters, new_rows, new_cols）
data_format="channels_last"：（batch, new_rows, new_cols, filters）

### Conv3D
三维卷积层
如input_shape=(128,128,128,1)时，表示对128幅128x128的单通道图像进行卷积操作。

**input shape**
data_format="channels_first"：（samples, channels, conv_dim1, conv_dim2, conv_dim3）
data_format="channels_last"：（samples, conv_dim1, conv_dim2, conv_dim3, channels）

**output shape**
data_format="channels_first"：（samples, filters, new_conv_dim1, new_conv_dim2, new_conv_dim3）
data_format="channels_last"：（samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, filters）

### Cropping1D
在时间轴（轴1）上对1D输入序列进行裁剪

**参数**
cropping: 整数或长度为2的tuple，表示在序列的开始和结尾的裁剪数量

### Cropping2D
对输入在二维空间上进行裁剪，如图片的长、宽

### Cropping3D
对三维数据进行裁剪

### UpSampling1D
对1D输入数据进行上采样。对每个时间部数据复制size次

### UpSampling2D
对二维数据进行上采样

### UpSampling3D
对三维数据进行上采样

### ZeroPadding1D
对一维数据的首位填充0，通常用于控制向量长度。

### ZeroPadding2D
对二维数据的上下左右进行0填充

### ZeroPadding3D
对三维数据的进行0填充

## 池化层
### MaxPooling1D
对1D数据进行最大值池化

### MaxPooling2D
对2D数据进行最大值池化

### MaxPooling3D
对3D数据进行最大值池化

### AveragePooling1D
对1D数据进行平均值池化

### AveragePooling2D
对2D数据进行平均值池化

### AveragePooling3D
对3D数据进行平均值池化

### GlobalMaxPooling1D
对1D数据进行全局最大值池化

**input shape**
(batch_size, steps, features)

**output shape**
(batch_size, features)

### GlobalAveragePooling1D
对1D数据继续拧全局平均值池化

### GlobalMaxPooling2D
对2D数据进行全局最大值池化

### GlobalAveragePooling2D
对2D数据继续拧全局平均值池化

## 局部连接层
### LocallyConnected1D
1D数据局部连接层，和Conv1D类似，只是每个核的权重不进行共享，也即是在输入数据不同位置的卷积操作权重不一样

### LocallyConnected2D
1D数据局部连接层，和Conv2D类似，只是每个核的权重不进行共享，也即是在输入数据不同位置的卷积操作权重不一样

## 循环层
### RNN
```
keras.layers.RNN(cell, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False)
```
所有循环神经网络的基类

**参数**
1. cell: RNN cell实例，具有如下内容
	1. call(input_at_t, states_at_t)方法：返回(output_at_t, states_at_t_plus_1)
	1. state_size属性：可以有以下三种取值
		1. 单个整数：循环次数（和output shape一致）
		1. 整数的list/tuple：每个state一个size，state_size[0]和output shape一致
	1. cell也可以是RNN cell实例的list，每个cell会依次排列
1. return_sequences：Boolean，返回输出序列的最后一个值还是所有值
1. return_state：Boolean，返回output的同时是否返回最终state
1. go_backwards：Boolean（默认为False），如果为True，则逆序输入input序列，并返回反转后的序列
1. stateful：Boolean（默认False），如果为True，则具备状态记忆，在一个batch的index i位置的最终状态，会作为下一个batch的indexi位置的初始值
1. unroll：Boolean（默认False），如果为True，则会把循环层展开层多个普通层，负责进行符号化的循环。展开可以加速RNN的速度，但是会占用更多的内存，因此只适用于较短的序列。
1. input_dim：输入的维数，在第一层时，input_dim或input_shape是必须的
1. input_length：当输入序列长度固定时，需要input_length指定输入序列的长度，在RNN后接Flatten然后接Dense时，必须指定input_length（否则dense的output shape无法计算），如果RNN不是网络的第一层，则需要在第一层通过input_shape制定input_length

**input_shape**
(batch_size, timesteps, input_dim)

**output shape**
如果return_state=True，则返回tensor的list，第一个tensor是output，后面的tensor为最终state。
如果return_sequences=True，则output为(batch_size, timesteps, units)
如果return_sequences=False，则output为(batch_size, units)

**输入数据屏蔽**
使用Embedding层设置mask_zero为True，可以屏蔽输入数据的某些时间步

**使用状态记忆RNN的注意事项**
当设置stateful=True，可以将RNN设置为stateful，当前batch训练后的最终状态作为下一batch的初始状态。对于stateful的网络，需要制定固定的batch size，在Sequential模型中，

### SimpleRNN
### GRU
### LSTM
### ConvLSTM2D
### SimpleRNNCell
### GRUCell
### LSTMCell
### StackedRNNCells
### CuDNNGRU
### CuDNNLSTM
