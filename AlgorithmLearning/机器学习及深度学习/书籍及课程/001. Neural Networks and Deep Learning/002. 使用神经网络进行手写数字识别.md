# Using neural nets to recognize handwritten digits
[原文链接](http://neuralnetworksanddeeplearning.com/chap1.html#the_architecture_of_neural_networks)

我们大脑皮层可以快速处理图像数据，大脑处理图像部分，仅仅V1层就有1.4亿个神经元，后面还跟着V2、V3、V4、V5等皮层进行更复杂的图像处理。

要使用电脑编程识别如下手写字体是一件很困难的事
![](http://neuralnetworksanddeeplearning.com/images/digits.png)
比如描述数字“9”为一个圆，下面接一竖，要通过传统编程制定这么一系列规则是非常困难的

神经网络使用与传统编程不同的方法来解决这个问题：

1. 获取大量的手写训练数据；
1. 开发一个可以从这些训练数据中进行学习的程序；

因此，神经网络可以使用这些手写训练样本来自动发现内在规则。如果提供更多的驯良样本，算法还能进一步提高识别准确度。

`在本章中，将通过74行代码，并且不需要使用额外的神经网络库，就可以是想一个手写数字识别系统。`仅仅这么简单一个程序，对手写数字的识别率就可以达到96%以上。在后续章节中，我们还将进一步优化算法，使其识别率达到99%以上。

本书首先从难度不高，但实用性强的手写数字识别开始，并且将讨论如何将这其中的方法运用到其他机器人视觉、语音识别、自然语言处理等领域。

同时，本章不仅仅为了实现一个手写数字识别程序，我们同时还将介绍神经网络的一些核心概念，如感知机（perceptron）和Sigmoid神经元，以及神经网络的训练方法，随机梯度下降。并且在全书中，作者将着重讲解“为什么”，而不是仅仅介绍“怎么做”

## perceptron（感知机）
感知机是上世纪50年代提出的，现在用得较多的是另一种神经元Sigmoid（下文将会介绍）。但在这之前，为了更好的理解sigmoid。我们将先认识什么是感知机。

感知机结构如下图，他的输入是多个0/1的数字，然后输出1个0/1结果

![](http://neuralnetworksanddeeplearning.com/images/tikz0.png)

神经元上对每个输入有一个权重，并由此决定了输出

$$output=\left\{\begin{matrix}
0&if\sum_jw_jx_j \leqslant threshold\\ 
1&if\sum_jw_jx_j > threshold
\end{matrix}\right.$$

这就是感知机工作的这个过程。

决策树就是一个将所有条件进行加权和的简单决策机制。
举个例子，当我们要决定是否参加一个晚会时，有以下考虑因素：

1. 天气是否好？（x1）
1. 女朋友（男朋友）是否想去？（x2）
1. 交通是否方便？（x3）

每个因素考虑不同权重时，可以得到不同的决策方案。

感知机并不能真是反馈大脑的决策过程，但是通过组合多层感知机，可以做出复杂的决策。

![](http://neuralnetworksanddeeplearning.com/images/tikz1.png)

上图网络中，感知机的第一层通过权重参数，做出了简单决策，第二层根据第一层的输出结构进行决策，因此第二层可以比第一层做出更复杂更抽象的决策。第三层可以做出更抽象的决策。因此使用多层网络可以做出复杂的决策。

$\sum_jw_jx_j>threshold$的表示方式比较复杂，可以对此进行化简。$\sum_jw_jx_j$使用点乘表示成以下格式$w\cdot x\equiv \sum_jw_jx_j$，其中$w$和$x$是权重和输入的向量表示，然后使用偏置b表示-threshold，并移动到同一侧，由此可以将感知机表示成以下形式：


$$
output=\sum \left\{\begin{matrix}
0 & if w\cdot x+b\leq 0\\ 
1 & if w\cdot x+b> 0
\end{matrix}\right.
$$
偏置可以想象成描述感知机输入1的难易程度，b越大，感知机越容易输出1。引入偏置b虽然是一个小小的改变，但是在后续的使用中，却会带来很大的便利。因此在本书以后的内容中，都不使用threhold，而是用偏置b（bias）。

感知机除了用于基于权重的决策外，应该一个用途是可以用来模拟电路逻辑单元，也即是与门、或门、与非门。比如有如下感知机，2个输入，权重均为-2，偏置为3：
![](http://neuralnetworksanddeeplearning.com/images/tikz2.png)

在这个感知机中，输入00输出1（-2*0-2*0+3=3），类似的，输入01、10都输出1，输入11输出0，因此这个感知机是一个与非门（NAND gate）。因为基于与非门可以搭建出任何逻辑运算，因此，感知机经过组合，可以处理任何逻辑运算问题。
如以下基于与非门的逻辑电路可以用来处理2个二进制位的加法。carry bit是进位（当x1、x2都是1时，carry bit为1）
![](http://neuralnetworksanddeeplearning.com/images/tikz3.png)

可以将上图中的与非门替换成上文的感知机：
![](http://neuralnetworksanddeeplearning.com/images/tikz4.png)

上图的感知机中，第一层的感知机的输出作为输入，两次输入到了下一层的同一个神经元，这并不是一种常规用法，因此如果需要避免这种情况，可以使用一个输入权重为-4的神经元。
![](http://neuralnetworksanddeeplearning.com/images/tikz5.png)

目前为止，我们将输入表示成$x_1$，$x_2$的变量，实际上我们可以增加一个额外的输入层：
![](http://neuralnetworksanddeeplearning.com/images/tikz6.png)

这个额外的输入层并不是真的感知神经元，只是输入的另一种写法。

感知机可以用于任何计算，一方面能力非常强大，另一方面感知机用在大型系统中，构建起来很困难。但是通过学习算法可以自动调整感知器的权重和偏置，这样就为感知机在复杂系统中的使用提供了可能性。

## Sigmoid neurons
学习算法听上去很强大，但是在神经网络中如何设计这样的算法呢？假设感知机网络的输入是手写数字图像，我们想让网络能学习调整权重和偏置，以使网络的输出是正确分类的数字。为了解学习过程如何实现，首先我们假设在权重或偏置上记性了微小调整，我们希望这些微小调整能够在输出上相应产生微小变化，下文中马上就能看到，这一特性使得算法的自动学习成为可能。

![](http://neuralnetworksanddeeplearning.com/images/tikz8.png)

如果对权重或偏置的轻微调整，仅仅在输出上产生微小变化，那么我们可以利用这一特性来修改权重和偏置，是网络输出我们想要的结果，比如说当网络将9的图像错误认为是8，我们希望能通过轻微调整权重和偏置，是网络能够将输出结构朝9靠近一点点，然后重复这个操作，当权重和偏置调整到足够好时，网络就成功学会了对手写数字进行识别。

但感知机却不具有这种轻微变化的特性，当权重和偏置发生了很小的变化时，有可能使神经元输出直接从0便到了1，从而使整个网络结果发生很大偏差。因此我们需要引入一个新的神经元结构sigmoid neuron。sigmoid和感知机很相似，但是改动了一小部分，使得轻微的权重和偏置的改变仅仅产生铣削的输出变化。

![](http://neuralnetworksanddeeplearning.com/images/tikz9.png)

与感知机对比，sigmoid有以下区别：
1. 输入不限于0/1，而是可以仍以0-1的实数
1. 输出也不是0/1，而是$\sigma(w\cdot x+b)$，其中$\sigma$叫做sigmoid函数，定义如下：
$$
\sigma \equiv \frac{1}{1+e^{-z}}
$$

因此在sigmoid中，输出与输入的计算公式可以写成以下格式：

$$
\frac{1}{1+exp(-\sum_jw_jx_j-b)}
$$

上式看上去很复杂，但是可以对之加以理解，假设$z\equiv w \cdov x+b$是一个较大的数，那么$e^{-z}\approx 0$，所以$\sigma(z)\approx 1$，当$z$是一个很大的正数时，输出接近1（和感知机结果相似），同理当$z$是一个很大的负数时，输出接近0，也和感知机结果相似。与感知机不同的是，sigmoid具有连续的中间值输出，在坐标系中可以对sigmoid作图如下：

![](002. 使用神经网络进行手写数字识别/sigmoid function.png)

如果$\sigma$是如下形式阶跃函数，则sigmoid就变成了感知机：
![](002. 使用神经网络进行手写数字识别/step function.png)

$\sigma$函数的连续性，是的权重和偏置的微小变化$\Delta  w$、$\Delta b$只会产生微小的输出变化$\Delta output$，并且可以从理论上推导出以下公式：

$$
\sum_{j}\frac{\partial output}{\partial w_j}\Delta w_j+\frac{\partial output}{\partial b}\Delta b
$$

从公式看出，$\Delta output$由output对$w_j$的偏微分，output对b的偏微分，以及各自变化量计算得到，并且$\Delta output$是$\Delta  w$和$\Delta b$的线性方程，因此可以更容易的了解权重和偏置变化如何影响输出。

在神经元中，我们主要利用了$\sigma$的连续性，而和它具体代数式无关，因此在后续章节还将看到，`可以采用其他形式的激活函数`$f(w\cdot x+b)$，不同激活函数产生的影响是上式output对$w_j$和b的偏微分不同。我们在使用时，用得最多的仍然是$\sigma$激活函数，因为指数在微分计算中可以大大降低计算难度。

sigmoid神经元的输出为0-1之间的连续实数，而不是简单的0/1，因此为输出提供了更多可能性。比如判断图像是否为9时，感知机只能给出是/否两种答案，而sigmoid可以判断出图像与9的相似程度。

## 练习
* 使用sigmoid模拟感知机，part 1
	
	假设将感知机中所有权重、偏置同时乘以一个大于0的常数c，证明：网络的output保持不变。
	
	`答`：感知机输出为1时，有如下形式$\sigma \cdot x+b> 0$，明显在左端同时乘以一个大于0的常数，不影响等式成立
	
* 使用sigmoid模拟感知机，part 1
	
	假设仍然在感知机中，输入数据$x$固定不变，有这么一组$w$和$b$并且对所有感知机$w \cdot x+b\neq 0$。现在将网络中所有感知机替换成sigmoid神经元，并将所有权重、偏置同时乘以一个大于0的常数c，证明：当$c\Rightarrow \infty$时，sigmoid与感知机输出一致，当有一个感知机的$w \cdot x+b=0$的话，结论是否成立？
	
	`答`：sigmoid公式如下$\sigma \equiv \frac{1}{1+e^{-z}}$，当$z=c(w \cdot x+b)$，当z>0时，如果c无穷大，则$\sigma$输出1，反之输出0，与感知机一致，当其中有感知机$w \cdot x+b=0$时，无论c如何变化$\sigma$都输出1，而感知机无此输出，因此最终结果可能不同。
	
## 神经网络结构

这一节中，将学习一个可以用来进行手写字体识别的神经网络，首先介绍一下神经网络的基本知识：

![](http://neuralnetworksanddeeplearning.com/images/tikz10.png)

1. 第一层叫输入层（input layer），其中的神经元叫做输入神经元（input neurons）
1. 最右边为输出层（output layer），其内的是输出神经元（output neurons）
1. 中间层为隐含层（hidden layer）

![](http://neuralnetworksanddeeplearning.com/images/tikz11.png)

上图神经网络具有2个隐含层。因为历史原因，上图的神经网络又是也叫做多层感知机（multilayer perceptrons，MLPs），但是实际采用的是sigmoid神经元而不是感知机，因此本书将不会采用这种说法。

输入层和输出层的设计比较简单、直观，比如想要判断一张64x64的图片内容是否是数字9，那么将网络输入层设计为64x64=4096个神经元，输出层仅1个神经元，当输入0.5以上值时，表示图像为9，否则不是9。

和输入、输出层设计的简单直观不同，隐含层的设计困难许多，也没办法简单的用几条规则来说明隐含层的设计原则，有一些隐含层的设计原则、经验，在书的后续内容中会有所介绍。

在之前的介绍中，神经元输出向下逐层传递，因此叫做`前馈神经网络（feedforward neural networks）`，也即是在网络机构中不会有输出反向作用于输入的循环出现。但是还有另一种网络结构：`循环神经网络（recurrent neural networks）`，在循环神经网络中，神经元会在某段时间内激活，并且在间隔一定时间后让其他神经元激活一段时间，在循环神经网络中，神经元的输出并不是立即作用于输入，而是需要一定的反应时间，循环神经网络更接近大脑工作方式，因此可以解决很大需要大型前馈神经网络才能解决的问题，但是相应的使用难度也大于前馈神经网络，本书主要将的是前馈神经网络。


### 简单的手写数字识别网络
要对手写数字序列进行识别，首先需要将数字序列拆分成一个一个独立的数字，然后再一次对每个数字进行识别。
![](http://neuralnetworksanddeeplearning.com/images/digits.png)
![](http://neuralnetworksanddeeplearning.com/images/digits_separate.png)
要晚上以上工作，我们主要集中在对独立数字的识别上，因为数字序列的拆分可以有多种方法实现，其中一种方法是采用多个方法对数字序列进行拆分，并将拆分后的独立数字进行识别，如果对每个独立数字的识别置信度都很高，说明拆分准确，如果对很多独立数字的识别结果模棱两可的，那说明拆分方法不够准确。

为了对独立的数字进行识别，我们采用了一个三层的神经网络：
![](http://neuralnetworksanddeeplearning.com/images/tikz12.png)	

上图神经网络介绍如下：
1. 训练样本为28x28的图像，因此输入层为28x28=784个神经元，其值为0-1的灰度值，0表示白色，1表示黑色。
1. 隐含层神经元个数设为n，并且后续将测试n在不同取值时的效果，上图中n=15。
1. 输入层共10个神经元，标记为0-9，通过输出层神经元的输出值，看那个数具有最大的输出结果。

看到这儿可能会质疑，为什么输出神经元需要10个，而不是4个，毕竟$2^4=16$，通过二进制的方式足够表示0-9的结果，但是根据实际经验，在这个问题中10个输出神经元比4个输出神经元能获得更好的效果，然后可能又有进一步疑惑，为什么10个神经元比4个神经元识别效果更好？是否能从中学到某种规律？

未解答以上疑惑，可以有助于了解神经网络的工作过程。首先考虑10个输出神经元的情况，第一个神经元是决定图像是否代表数字0，其依据是对隐含层神经元输出的加权，那隐含层又是完成什么任务呢？假设隐含层第一个神经元作用是判断图像是否具有如下特征：
![](http://neuralnetworksanddeeplearning.com/images/mnist_top_left_feature.png)

也即是隐含层第一个神经元对具有如上图所示特征的图形输出较大值，而不具有上述特征的图像输出较小值。再假设隐含层第二三四个神经元纷纷识别一下特征：
![](http://neuralnetworksanddeeplearning.com/images/mnist_other_features.png)


然后当结合以上四个神经元，可以形成类似0的数字:
![](http://neuralnetworksanddeeplearning.com/images/mnist_complete_zero.png)

所以当以上四个神经元都处于激活状态时，可以推测图像代表数字0。而对于4个输出神经元，第一个神经元需要判断哪些隐含层神经元最重要，这个过程比10个神经元复杂很多。

以上猜想仅仅是神经网络可能的工作过程，但是通过学习，也有可能找到一种使用4个输出神经元能达到更好效果的参数，但是作为一种经验结论，采用上文描述的思考方法，在设计神经网络时，提供了一种思路，可以节省网络设计时间。

#### 练习
一种验证4个输出神经元二进制表示的方式是在上文神经网络输出层后面追加一个具有4个神经元的输出层，假设前三层神经元已经在上文中正确训练，并能正确识别0-9的数字，尝试着找到第三层到新增加的输出层的权重和偏置，使新的输出层也能正确工作。
![](http://neuralnetworksanddeeplearning.com/images/tikz13.png)

`答`：需要找到权重和偏移，产生类似这样的结果（0000000001--0001，0000000010--0010，0000000100--0011，00000010-00-0100，。。。）

### 使用梯度下降进行学习
设计好神经网络后，如何使用上文的神经网络进行数字识别呢？首选需要数据集来进行学习——训练集。[MNIST](http://yann.lecun.com/exdb/mnist/)有大量手写数字以及标注可以使用。MNIST数据集有2部分：
1. 60000张图像用于训练。每张图像为28x28的灰度图
1. 10000张图像用于测试。同样是28x28的灰度图


接下来我们使用x表示一组训练数据，将x看作一个784维向量（28x28=784），向量中每个值表示一个图像像素的灰度值，再使用$y=y(x)$表示输出，y是一个10维向量，如对于数字6，$y(x)=(0,0,0,0,0,1,0,0,0)^T$是网络的准确输出，这儿的T是转置，将行向量变成列向量。

神经网络的目标是要找到合适的权重和偏置，是的对于所有的输入x都能得到正确的输出$y(x)$，为了表示权重和偏置的合适程度，我们接下来引入损失函数：
$$
C(w,b)\equiv \frac{1}{2n}\sum_x\begin{Vmatrix}y(x）-a\end{Vmatrix}^2
$$
上式中，w表示权重集合，b表示偏置集合，n是训练样本数量，x是输入向量，a是输入向量对应的输出向量。符号$\begin{Vmatrix}v\end{Vmatrix}$表示的是向量长度。C叫做二次损失函数，或者叫均方差。

从上式可以看出，$C(w,b)$非负，并且当网络与输出约接近，C的值越小，所以训练目标可以变成找到合适的w、b，使得$C(w,b)$的取值尽量小，直至为0，为完成这个目标，需要使用叫做`梯度下降(gradient descent)`的算法。

为什么将目标定为最小化损失函数，而不将目标定为最大化识别正确率，主要原因是识别正确率和权重、偏置之间并不是连续关系，权重、偏置的轻微改变并不能立即反映到识别正确率上，也就很难对此进行优化。那如果同样是连续函数，为什么不选择其他函数而选择均方差，在以后的章节，会尝试这修改损失函数，但这儿，对于了解神经网络的基本工作过程，使用均方差更容易理解，并且也能取得足够好的结果。

现在，不考虑上文的神经网络结构、$\omiga$，MNIST、损失函数等等概念，仅仅考虑一个由多个变量组成的函数，并且目标是最小化这个函数，可以使用梯度下降方法来解决这个问题。假设需要最小化$C(v)$，现在考虑v由2个变量$v_1,v_2$组成的情况：
![](http://neuralnetworksanddeeplearning.com/images/valley.png)

从上图可以使用肉眼直观看出C取到最小值的点，而对于多个变量组成的函数，用肉眼没法看出最小取值。一个方法是使用代数计算，通过计算微分，并判断C的极致，来取得最小值，但是对于变量数较少时可能有用，但是在神经网络中，我们通常希望变量数越多越好，甚至达到数十亿变量，代数计算显然行不通。

代数计算行不通，但任然可以寻找其他方法，将函数图像想象成一个峡谷，一个小球在峡谷斜面上自由滚动，对于一小块平面，小球会向较低方向滚动，根据经验，最终小球会滚到峡谷最低端，我们是否可以借鉴这样的操作的？首先随机选择一个起始点，模拟小球滚动到谷底的操作，可以对C进行求导（也有可能二次求导），求导结果可以反映出函数的局部形状走势，然后小球就可以向局部较小位置滚动。

更进一步讨论，假设小球在$v_1,v_2$方向分别进行了轻微移动$\Delta v_1,\Delta v_2$，可以通过计算求得C的变化量：
$$
\Delta C\approx \frac{\partial C}{\partial v_1}\Delta v_1+\frac{\partial C}{\partial v_2}\Delta v_2
$$
需要找到合适的$\Delta v_1,\Delta v_2$使得$\Delta C$为负，也即是小球向低处滚动。定义$\Delta v$为v的变化向量$\Delta v\equiv (\Delta v_1,\Delta v_2)^T$，定义C的梯度$\triangledown C$为偏微分向量$(\frac{\partial C}{\partial v_1},\frac{\partial C}{\partial v_2})^T$
$$
\triangledown C\equiv (\frac{\partial C}{\partial v_1},\frac{\partial C}{\partial v_2})^T
$$
然后得到如下简写

$$
\Delta C \approx \triangledown C \cdot \Delta v
$$
上面公式也能反应为什么$\triangledown C$叫梯度，因为它反应了C随着v的变化关系。根据上式，任何确保$\Delta C$为非正值呢，实际上只需要选择
$$
\Delta v=-\eta \triangledown C
$$
其中$\eta$是一个小的正数（也叫做`学习率learning rate`），就能确保$\Delta C$为非正值。

因此按照如下公式逐步改变$v$，可以确保$C$的不断减小：
$$
v'=v+\Delta v=v-\eta \triangledown C
$$

总结下来，梯度下降是通过重复计算梯度$\triangledown C$的值，然后按照梯度的反方向进行移动，从而像山谷底部不断逼近。示意图如下：
![](http://neuralnetworksanddeeplearning.com/images/valley_with_ball.png)

在实际使用时，学习率$\eta$的取值太大可能造成不收敛，太小收敛太慢，因此学习率取值需要多尝试。

在上文中，$C$是2个变量的函数，但实际在多变量函数中，梯度下降仍然能够正常工作，此时

$$
\Delta v = (\Delta v_1,...,\Delta v_m)^T
$$
$$
\triangledown C\equiv (\frac{\partial C}{\partial v_1},...,\frac{\partial C}{\partial v_m})^T
$$
$$
\Delta C \approx \triangledown C \cdot \Delta v
$$
和两个变量的情形相同，可以选择：
$$
\Delta v=-\eta \triangledown C
$$
梯度下降也许是最好的最小值搜索策略，因为可以证明，当变量的改变距离一定时，也即是$\left \| \Delta v \right \|=\epsilon>0$时，当按照梯度方向$\Delta v=-\eta \triangledown C$时，可以使C的值尽量小。

#### 练习
1. 证明本节最后一段话。提示：可能用到[Cauchy-Schwarz inequality](http://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality)
1. 本节证明了当C时两变量或多个变量函数时的梯度下降，当C是一个变量的函数时，能否尝试理解并接受C的梯度下降。

### 梯度下降的应用
在神经网络中，使用梯度下降来训练权重和偏置，进而最小化损失函数。将C看做以权重、偏置$w_k$、$b_l$为变量的函数，可以得到
$$
\triangledown C\equiv (\frac{\partial C}{\partial w_k},\frac{\partial C}{\partial b_l})^T
$$
因此可以得到：
$$
\begin{matrix}
w_k\rightarrow w'_{k} = w_k-\eta \frac {\partial C}{\partial w_k}
\\ 
b_l\rightarrow b'_{l} = b_l-\eta \frac {\partial C}{\partial b_l}
\end{matrix}
$$

不断重复这个过程，理论上可以找到损失函数的最小值，从而可以将梯度下降用于神经网络的训练。

将梯度下降运用于神经网络训练时，还有许多问题需要讨论，首先有这么个问题。有损失函数公式：
$$
C(w,b)\equiv \frac{1}{2n}\sum_x\begin{Vmatrix}y(x）-a\end{Vmatrix}^2
$$
可以写作
$$
C\equiv \frac{1}{n}\sum_xC_x
\\
C_x\equiv \frac{\begin{Vmatrix}y(x）-a\end{Vmatrix}^2}{2}
$$
对于每个训练数据，都需要计算器梯度，并进行平均，当训练数据量很大时，训练过程非常缓慢。因此可以使用一种叫做随机梯度下降（stochastic gradient descent）的方法来加速训练过程，其核心思想是通过随机挑选部分训练数据计算其梯度$\triangledown C_x$，通过计算随机样本梯度的平均值来估算总的梯度$\triangledown C$，进而达到训练加速的目的。

进一步分析随机梯度下降的具体实现方法，随机挑选m个随机样本，假设为$X_1,X_2,...,X_m$，将其作为一批，当m选得足够大时，可以认为$\triangledown C_{x_j}$足够表示$\triangledown C$也即是：
$$
\frac{\sum_{j=1}^{m}\triangledown C_{x_j}}{m} \approx 
\frac{\sum_{x}\triangledown C_{x}}{n} = \triangledown C
$$
其中第二项是所有训练数据的梯度，交换一下格式，即为：
$$
\triangledown C \approx \frac{1}{m}\sum_{j=1}^{m}\triangledown C_{x_j}
$$
将以上分析和神经网络训练相结合，有：
$$
\begin{matrix}
w_k\rightarrow w'_{k} = w_k-\frac{\eta}{m}\sum_j\frac {\partial C_{X_j}}{\partial w_k}
\\ 
b_l\rightarrow b'_{l} = b_l-\frac{\eta}{m}\sum_j\frac {\partial C_{X_j}}{\partial b_l}
\end{matrix}
$$
当挑选了一批数据进行训练后，继续挑选下一批数据，直到所有数据都参与了训练，这么一次完整训练叫做一代（epoch）训练，一次epoch训练完成后，开始下一次epoch训练。

比如对于MNIST数据集，总的训练数据n=60000，如果选择小批数量m=10，那么计算梯度时能有6000倍的加速。随机梯度下降之所有有效，是因为我们在训练神经网络时，不需要知道精确的梯度方向，只需要计算出大致不出错的方向，就可以完成模型的调优。在实际使用过程中，随机梯度下降是神经网络广泛使用并且功能强大的训练方法，本书中也使用随机梯度下降进行训练。

#### 练习
* 随机梯度下降的极端情况是使小批数量为1，这时没得到一个训练输入，就通过训练规则改变一次权重和偏置，这种训练方法称为在线训练或增强学习。在线训练中，网络一次只需要一组训练数据（人的处理方式也是这样），可以列举几条在线训练与小批不为1（如20）的随机梯度下降相比的优点和缺点。

> 在学习梯度下降时，我们说过多维数据可以想象成多维空间的曲面，但是即使数学教授也可能难以想象出三维以上的曲面效果，因此需要借助代数工具来辅助理解

## 使用神经网络进行数字识别
接下来使用Python2.7编写一个仅仅74行的程序，使用随机梯度下降和MNIST数据进行手写数字识别。

首先下载训练数据

	https://github.com/mnielsen/neural-networks-and-deep-learning.git

原始的MNIST数据集有60000张训练图像，10000张测试图像，我们对数据进行了一些拆分：测试数据不变，60000训练数据分成50000张训练数据，10000张validation集（其中validation集用于后续优化学习率等参数，这一章暂时用不到），后面我们再讲训练数据时，实际讲的是拆分后的50000张图片。

除MNIST数据外，还需要Python的Numpy库来进行线性代数计算。

首先创建一个表示神经网络的类Network，以下代码对网络进行初始化：

```
class Network(object):
    def __init__(self, sizes):
        self.num_layers = len(sizes)
        self.sizes = sizes
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.weights = [np.random.randn(y, x) 
                        for x, y in zip(sizes[:-1], sizes[1:])]
```
其中sizes表示各层的神经元个数，如net = Network([2,3,1])表示2个输入神经元，3个隐藏神经元，1个输出神经元。

biases和weights都通过np.random.randn进行随机初始化，其中第一层为输入层，不需要biases。biases和weights都是numpy的矩阵数据，如net.weights[1]是指第二层神经元到第三层神经元的权重。以$w_{jk}$表示第二层的第k个神经元与第三层第j个神经元，可以有：
$$
a'=\sigma (wa+b)
$$
其中a是第二层神经元的输出，$\sigma$是sigmoid函数$a'$是第三层神经元的输出。

### 练习
写出上式$a'$的完整形式

### Python代码
编写sigmoid函数计算网络输出：

```
def sigmoid(z):
	return 1.0/(1.0+np.exp(-z))
```
然后编写Network类的feedforward函数，通过输入上一层结果啊，返回下一层输出。

```
def feedforward(self, a):
	"""Return the output of the network if "a" is input."""
	for b, w in zip(self.biases, self.weights):
		a = sigmoid(np.dot(w, a)+b)
	return a
```
然后为了让Network具有学习能力，编写SGD（随机梯度下降）函数：

```
def SGD(self, training_data, epochs, mini_batch_size, eta,
            test_data=None):
	"""Train the neural network using mini-batch stochastic
 gradient descent.  The "training_data" is a list of tuples
 "(x, y)" representing the training inputs and the desired
 outputs.  The other non-optional parameters are
 self-explanatory.  If "test_data" is provided then the
 network will be evaluated against the test data after each
 epoch, and partial progress printed out.  This is useful for
 tracking progress, but slows things down substantially."""
	if test_data: n_test = len(test_data)
	n = len(training_data)
	for j in xrange(epochs):
		random.shuffle(training_data)
		mini_batches = [
			training_data[k:k+mini_batch_size]
			for k in xrange(0, n, mini_batch_size)]
		for mini_batch in mini_batches:
			self.update_mini_batch(mini_batch, eta)
		if test_data:
			print "Epoch {0}: {1} / {2}".format(
				j, self.evaluate(test_data), n_test)
		else:
			print "Epoch {0} complete".format(j)
```
上述代码中，输入数据是（x,y）的元组，分别表示输入和标注的输出。epochs表示完整训练次数，每次完整训练，先将数据按照mini_batch_size进行拆分，然后将mini_batch分别送入self.update_mini_batch函数进行训练，如果有test_data，则每次完整训练后，将大于出当前测试数据结果。

self.update_mini_batch的作用是对mini_batch中的数据，应用梯度下降的反向传播算法（backpropagation）进行权重和偏置的优化：

```
def update_mini_batch(self, mini_batch, eta):
    """Update the network's weights and biases by applying
gradient descent using backpropagation to a single mini batch.
The "mini_batch" is a list of tuples "(x, y)", and "eta"
is the learning rate."""
    nabla_b = [np.zeros(b.shape) for b in self.biases]
    nabla_w = [np.zeros(w.shape) for w in self.weights]
    for x, y in mini_batch:
        delta_nabla_b, delta_nabla_w = self.backprop(x, y)
        nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
        nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
    self.weights = [w-(eta/len(mini_batch))*nw 
                    for w, nw in zip(self.weights, nabla_w)]
    self.biases = [b-(eta/len(mini_batch))*nb 
                   for b, nb in zip(self.biases, nabla_b)]
```
update_mini_batch的工作是对mini_batch中的每一个训练样本计算梯度，并调整权重和偏置。其核心是self.backprop，backpropagation算法将在下章详细介绍，这儿仅使用。

完整的代码[here](https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py)

运行如下代码，可以得到每epoch的训练结果：

```
import mnist_loader
import network
training_data, validation_data, test_data = mnist_loader.load_data_wrapper()

net = network.Network([784,30,10])
net.SGD(training_data,30,10,3.0,test_data=test_data)
```
以上代码创建了一个具有30个神经元的网络，测试样本可以得到约95%的正确率，如果改为100个神经元，即改为：

```
net = network.Network([784,100,10])
net.SGD(training_data,30,10,3.0,test_data=test_data)
```
则可得得到月96%的正确率，修改神经元个数、学习率、epoch次数、mini_batch大小，都可能影响识别效果，如果是第一次接触的问题，我们可能会困惑是否学习率设置太大或太低？是否初始化不够好？是否训练数据不足？是否训练次数不够？甚至是否神经网络不能用于解决这个问题？等等，对神经网络的调试并不是一项简单的工作，除了需要经验外，还可以使用一些广泛测试过的网络结构，因此本书剩下部分将会引入一些常用的网络结构，以及如何选择上诉数字识别网络的参数。

### 练习
尝试创建一个没有隐含层的网络，也即是network.Network([784,10])，并尝试运行并试着理解。

### 效果对比
当我们说神经网络识别效果好时，需要与其他方法进行对比。第一种是完全随机，那么正确率理论上能有10%；另一种方法是判断黑点数量[code](https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/mnist_average_darkness.py)，如下如所示，2黑点数比1更多，以此可以得到约22.25%的正确率
![](http://neuralnetworksanddeeplearning.com/images/mnist_2_and_1.png)
还可以使用SVM进行识别，可以直接使用scikit-learn中集成的LIBSVM，可以得到月94.35%的正确率[code](https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/mnist_svm.py)

看上去SVM的正确率和神经网络差别不大，但在本书后续内容，将对神经网络进行优化，使其正确率能面向优于SVM。当然94.35%只是scikit-learn的默认svm参数，结果优化后的SVM可以达到98.5%。而使用神经网络进行识别的正确率在2013年已经达到了99.79%。

## 如何理解深度学习
上文的数字识别，程序通过训练样本自动学习了权重和偏置，并取得了非常好的识别效果，但是我们如何理解深度学习的工作原理呢？并且理解了其工作原理后，是否能对齐进一步改善呢？

以识别人脸为例，假设我们不使用上文的学习算法，而进行编码，我们可能想到的是将人脸识别分解成更小的子问题：图像是否有研究在左上方、右上方？是否有鼻子在中间？是否有嘴在下方？是否有头发在顶上？等等

当好几个子问题的答案都是“yes”时，我们可以推测是一张脸，也即是可以有这么一张网络用于判断人脸：
![](http://neuralnetworksanddeeplearning.com/images/tikz14.png)
对上面的子问题，我们仍然可以继续分解，比如要判断是否有眼睛在左上方，可以判断是否有眉毛？是否有睫毛？是否瞳孔？等：
![](http://neuralnetworksanddeeplearning.com/images/tikz15.png)
问题仍然可以继续分解，直到问题可以使用神经元简单的判断图像像素。

这样通过许多层网络结构，可以将人脸识别逐层分解，前面层处理简单的问题，后面层处理抽象的问题。这种具有2层或以上的神经网络称为：**深度神经网络**

在1980s到1990s间，人们使用随机梯度下降和反向传播来训练深度神经网络，但除了在一些特定网络外，其他训练效果均不好，知道2006年，在随机梯度下降和反向传播基础上，发展起来了一些列专门针对深度神经网络的新技术，从而使得深度神经网络取得较好的训练效果。