# 提升神经网络学习效果
[原文链接](http://neuralnetworksanddeeplearning.com/chap3.html)

## 前言
当我们学习高尔夫时，首先会画大量时间练习挥球杆的简单动作，同样的，反向传播算法是神经网络的基础，本章将介绍可以提升反向传播算法的一些方法。

本章包括以下内容：
1. 1种更好的损失函数，交叉熵（cross-entropy）
2. 4种回归方法（L1，L2回归，dropout，训练数据人工膨胀），使网络能够在现有训练数据下工作更好
3. 1中优化网络初始化权重的方法
4. 一系列选择网络超参数的启发式方法
5. 简要介绍其他技术

以上方法都比较独立，可以选择感兴趣的阅读，同时还会介绍一些代码优化技术，使上一张的数字识别更准确

## 交叉熵损失函数
当我们学习弹琴时，对出错的地方往往印象更加深刻，我们也希望神经网络能够更快的从错误中进行学习。以下图简单的网络为例：
![](http://neuralnetworksanddeeplearning.com/images/tikz28.png)

我们希望网络处理一件很简单的事：接受输入1，并输出0。当权重w为0.6，偏置b为0.9时，output=0.82，此时可以快速完成训练；但当权重w为2，偏置b为2时，output=0.98，此时开始学习的速度变得很慢。这和人的理解完全不一样，一般我们认为当错误越大时，学习速度应该越快，但是从上面例子可以看出，在神经网络中却不是如此。


